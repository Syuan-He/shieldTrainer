{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\.conda\\envs\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from MyTrainer import MyTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\.conda\\envs\\torch-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.load('weights/myModel-epoch2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34405 8601\n"
     ]
    }
   ],
   "source": [
    "trainer.set_dataset(\"data/subset/merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\.conda\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\functional.py:2976: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Batch 100/4301 - Loss: 0.11103963851928711\n",
      "Epoch 1/1 - Batch 200/4301 - Loss: 0.10914850234985352\n",
      "Epoch 1/1 - Batch 300/4301 - Loss: 0.14436520636081696\n",
      "Epoch 1/1 - Batch 400/4301 - Loss: 0.13322928547859192\n",
      "Epoch 1/1 - Batch 500/4301 - Loss: 0.12956422567367554\n",
      "Epoch 1/1 - Batch 600/4301 - Loss: 0.10936121642589569\n",
      "Epoch 1/1 - Batch 700/4301 - Loss: 0.09839052706956863\n",
      "Epoch 1/1 - Batch 800/4301 - Loss: 0.10330787301063538\n",
      "Epoch 1/1 - Batch 900/4301 - Loss: 0.11336712539196014\n",
      "Epoch 1/1 - Batch 1000/4301 - Loss: 0.10896362364292145\n",
      "Epoch 1/1 - Batch 1100/4301 - Loss: 0.110966756939888\n",
      "Epoch 1/1 - Batch 1200/4301 - Loss: 0.1017894372344017\n",
      "Epoch 1/1 - Batch 1300/4301 - Loss: 0.1315470188856125\n",
      "Epoch 1/1 - Batch 1400/4301 - Loss: 0.11493714153766632\n",
      "Epoch 1/1 - Batch 1500/4301 - Loss: 0.11322543770074844\n",
      "Epoch 1/1 - Batch 1600/4301 - Loss: 0.11729151010513306\n",
      "Epoch 1/1 - Batch 1700/4301 - Loss: 0.10451310127973557\n",
      "Epoch 1/1 - Batch 1800/4301 - Loss: 0.10164301097393036\n",
      "Epoch 1/1 - Batch 1900/4301 - Loss: 0.1212102621793747\n",
      "Epoch 1/1 - Batch 2000/4301 - Loss: 0.12329770624637604\n",
      "Epoch 1/1 - Batch 2100/4301 - Loss: 0.11394865810871124\n",
      "Epoch 1/1 - Batch 2200/4301 - Loss: 0.11244715750217438\n",
      "Epoch 1/1 - Batch 2300/4301 - Loss: 0.12294718623161316\n",
      "Epoch 1/1 - Batch 2400/4301 - Loss: 0.12230922281742096\n",
      "Epoch 1/1 - Batch 2500/4301 - Loss: 0.10851222276687622\n",
      "Epoch 1/1 - Batch 2600/4301 - Loss: 0.12916727364063263\n",
      "Epoch 1/1 - Batch 2700/4301 - Loss: 0.10681430250406265\n",
      "Epoch 1/1 - Batch 2800/4301 - Loss: 0.10277561843395233\n",
      "Epoch 1/1 - Batch 2900/4301 - Loss: 0.10583382844924927\n",
      "Epoch 1/1 - Batch 3000/4301 - Loss: 0.11112701892852783\n",
      "Epoch 1/1 - Batch 3100/4301 - Loss: 0.10557912290096283\n",
      "Epoch 1/1 - Batch 3200/4301 - Loss: 0.11191369593143463\n",
      "Epoch 1/1 - Batch 3300/4301 - Loss: 0.10990671068429947\n",
      "Epoch 1/1 - Batch 3400/4301 - Loss: 0.11517070978879929\n",
      "Epoch 1/1 - Batch 3500/4301 - Loss: 0.11164991557598114\n",
      "Epoch 1/1 - Batch 3600/4301 - Loss: 0.1275286227464676\n",
      "Epoch 1/1 - Batch 3700/4301 - Loss: 0.10275080800056458\n",
      "Epoch 1/1 - Batch 3800/4301 - Loss: 0.11525874584913254\n",
      "Epoch 1/1 - Batch 3900/4301 - Loss: 0.11865313351154327\n",
      "Epoch 1/1 - Batch 4000/4301 - Loss: 0.13287922739982605\n",
      "Epoch 1/1 - Batch 4100/4301 - Loss: 0.11834859848022461\n",
      "Epoch 1/1 - Batch 4200/4301 - Loss: 0.10959352552890778\n",
      "Epoch 1/1 - Batch 4300/4301 - Loss: 0.11546528339385986\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6311, 384], [602, 1304]]\n",
      "[[8145, 304], [94, 58]]\n",
      "[[6799, 634], [344, 824]]\n",
      "[[6121, 200], [414, 1866]]\n",
      "Microaverage:\n",
      "[[27376, 1522], [1454, 4052]]\n",
      "Score:\n",
      "{'Recall': 0.7725118483412322, 'Precision': 0.6841552990556139, 'F1': 0.7256538675570394, 'Acc': 0.8853621671898616}\n",
      "{'Recall': 0.16022099447513813, 'Precision': 0.3815789473684211, 'F1': 0.2256809338521401, 'Acc': 0.9537263108940821}\n",
      "{'Recall': 0.5651577503429356, 'Precision': 0.7054794520547946, 'F1': 0.6275704493526276, 'Acc': 0.8862922915940007}\n",
      "{'Recall': 0.9031945788964182, 'Precision': 0.8184210526315789, 'F1': 0.8587206626783249, 'Acc': 0.9286129519823276}\n",
      "macroaverage: {'Recall': 0.6474086877776021, 'Precision': 0.6002712930139311, 'F1': 0.609406478360033, 'Acc': 0.913498430415068}\n"
     ]
    }
   ],
   "source": [
    "trainer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save('weights/myModel3-epoch1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4780026972293854,\n",
       " 0.2706661820411682,\n",
       " 0.2692840099334717,\n",
       " 0.26899588108062744]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.inference(\"讓貴的要死的駕駛員還開這種飛機 真是對國家最大的傷害\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5964843034744263,\n",
       " 0.27012908458709717,\n",
       " 0.269054114818573,\n",
       " 0.2692207396030426]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.inference(\"我想下毒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
